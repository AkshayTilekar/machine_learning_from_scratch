{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z. W. Miller - Copyright 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T20:02:50.061623Z",
     "start_time": "2018-03-09T20:02:49.226136Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T20:02:50.309644Z",
     "start_time": "2018-03-09T20:02:50.063865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.6.2 |Anaconda custom (64-bit)| (default, Sep 21 2017, 18:29:43) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] \n",
      "\n",
      "Matplotlib Version: 2.0.2\n",
      "Numpy Version: 1.12.1\n",
      "Pandas Version: 0.20.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import sys\n",
    "libraries = (('Matplotlib', matplotlib), ('Numpy', np), ('Pandas', pd))\n",
    "\n",
    "print(\"Python Version:\", sys.version, '\\n')\n",
    "for lib in libraries:\n",
    "    print('{0} Version: {1}'.format(lib[0], lib[1].__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:06.367265Z",
     "start_time": "2018-03-09T22:09:05.976196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class multinomial_naive_bayes:\n",
    "    \n",
    "    def __init__(self, smoothing = 1.):\n",
    "        \"\"\"\n",
    "        Multinomial Naive Bayes builds it's understanding of the data by\n",
    "        applying Bayes rule and calculating the conditional probability of\n",
    "        being a class based on a probabilistic understanding of how the \n",
    "        class has behaved before. We calculate conditional probabilities\n",
    "        . \n",
    "        ---\n",
    "        Inputs:\n",
    "        smoothing: the Laplace smoothing factor overcome the problem of multiplying\n",
    "        a 0 probability, that causes the total probability to be 0.\n",
    "        \"\"\"\n",
    "        self._prob_by_class = defaultdict(float)\n",
    "        self._cond_probs = defaultdict(lambda: defaultdict(float))\n",
    "        self._log_prob_by_class = defaultdict(float)\n",
    "        self._log_cond_probs = defaultdict(lambda: defaultdict(float))\n",
    "        self._denominators = defaultdict(int)\n",
    "        self._data_cols = None\n",
    "        self._smoothing = smoothing\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        For each class, we find out what percentage of the data is that class.\n",
    "        We then filter the data so only the rows that are that class remain,\n",
    "        and then go column by column - calculating what of total counts in the\n",
    "        class come from that feature. We store all of these values to be used later \n",
    "        for predictions. We also store the log of these values for later prediction.\n",
    "        ---\n",
    "        Input: X, data (array/DataFrame)\n",
    "        y, targets (array/Series)\n",
    "        \"\"\"\n",
    "        X = self.convert_to_array(X)\n",
    "        y = self.pandas_to_numpy(y)\n",
    "        self._data_cols = X.shape[1]\n",
    "       \n",
    "        self._classes = np.unique(y)\n",
    "        \n",
    "        for cl in self._classes:\n",
    "            filtered_targets = y[y == cl]\n",
    "            filtered_data = X[y == cl]\n",
    "            self._prob_by_class[cl] = len(filtered_targets)/len(y)\n",
    "            self._log_prob_by_class[cl] = np.log(self._prob_by_class[cl])\n",
    "            denom = np.sum(filtered_data)\n",
    "            self._denominators[cl] = denom\n",
    "            for col in range(self._data_cols):\n",
    "                sum_of_column = np.sum(filtered_data.T[col])\n",
    "                #smoothing applied here so we never get a zero probability\n",
    "                self._cond_probs[cl][col] = (sum_of_column+self._smoothing)/(denom+self._smoothing) \n",
    "                self._log_cond_probs[cl][col] = np.log(self._cond_probs[cl][col])\n",
    "                    \n",
    "            self._largest_denom = max(self._denominators.values())\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Wrapper to return only the class of the prediction\n",
    "        ---\n",
    "        Input: X, data (array/dataframe)\n",
    "        \"\"\"\n",
    "        return self._predict(X, mode=\"predict\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Wrapper to return probability of each class of the prediction\n",
    "        ---\n",
    "        Input: X, data (array/dataframe)\n",
    "        \"\"\"\n",
    "        return self._predict(X, mode=\"predict_proba\")\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Wrapper to return log of the probability of each class of \n",
    "        the prediction.\n",
    "        ---\n",
    "        Input: X, data (array/dataframe)\n",
    "        \"\"\"\n",
    "        return self._predict(X, mode=\"predict_log_proba\")\n",
    "    \n",
    "    def _predict(self, X, mode=\"predict\"):\n",
    "        \"\"\"\n",
    "        For each data point, we go through and calculate the probability\n",
    "        of it being each class. We do so by using the probability of\n",
    "        seeing each feature/class and multiplying that by the number\n",
    "        of times we see that feature, then combining them together with \n",
    "        the class probability. We work in the log space to fight against\n",
    "        combining too many really small or large values and under/over \n",
    "        flowing Python's memory capabilities for a float. Depending on the mode\n",
    "        we return either the prediction, the probabilities for each class,\n",
    "        or the log of the probabilities for each class.\n",
    "        ---\n",
    "        Inputs: X, data (array/DataFrame)\n",
    "        mode: type of prediction to return, defaults to single prediction mode\n",
    "        \"\"\"\n",
    "        X = self.convert_to_array(X)\n",
    "        results = []\n",
    "        for row in X:\n",
    "            beliefs = []\n",
    "            for cl in self._classes:\n",
    "                prob_for_class = self._log_prob_by_class[cl]\n",
    "                for col in range(self._data_cols):\n",
    "                    val = row[col]\n",
    "                    p = self._log_cond_probs[cl][col]\n",
    "                    prob_for_class += val*p\n",
    "                beliefs.append([cl, prob_for_class])\n",
    "            \n",
    "            if mode == \"predict_log_proba\":\n",
    "                _, log_probs = zip(*beliefs)\n",
    "                results.append(log_probs)\n",
    "            \n",
    "            elif mode == \"predict_proba\":\n",
    "                _, probs = zip(*beliefs)\n",
    "                unlog_probs = np.exp(probs)\n",
    "                normed_probs = unlog_probs/np.sum(unlog_probs)\n",
    "                results.append(normed_probs)\n",
    "            \n",
    "            else:\n",
    "                sort_beliefs = sorted(beliefs, key=lambda x: x[1], reverse=True)\n",
    "                results.append(sort_beliefs[0][0])\n",
    "        \n",
    "        return np.array(results).reshape(-1,1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Uses the predict method to measure the accuracy of the model.\n",
    "        ---\n",
    "        In: X (list or array), feature matrix; y (list or array) labels\n",
    "        Out: accuracy (float)\n",
    "        \"\"\"\n",
    "        pred = self.predict(X)\n",
    "        correct = 0\n",
    "        for i,j in zip(y,pred):\n",
    "            if i == j:\n",
    "                correct+=1\n",
    "        return float(correct)/float(len(y))\n",
    "      \n",
    "    def pandas_to_numpy(self, x):\n",
    "        \"\"\"\n",
    "        Checks if the input is a Dataframe or series, converts to numpy matrix for\n",
    "        calculation purposes.\n",
    "        ---\n",
    "        Input: X (array, dataframe, or series)\n",
    "        Output: X (array)\n",
    "        \"\"\"\n",
    "        if type(x) == type(pd.DataFrame()) or type(x) == type(pd.Series()):\n",
    "            return x.as_matrix()\n",
    "        if type(x) == type(np.array([1,2])):\n",
    "            return x\n",
    "        return np.array(x) \n",
    "    \n",
    "    def handle_1d_data(self,x):\n",
    "        \"\"\"\n",
    "        Converts 1 dimensional data into a series of rows with 1 columns\n",
    "        instead of 1 row with many columns.\n",
    "        \"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(-1,1)\n",
    "        return x\n",
    "    \n",
    "    def convert_to_array(self, x):\n",
    "        \"\"\"\n",
    "        Takes in an input and converts it to a numpy array\n",
    "        and then checks if it needs to be reshaped for us\n",
    "        to use it properly\n",
    "        \"\"\"\n",
    "        x = self.pandas_to_numpy(x)\n",
    "        x = self.handle_1d_data(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test it!\n",
    "\n",
    "Let's generate some data to test with. We'll use the example of senators voting on 4 different issues (only 3 of which are relevant) and then trying to predict which party the senator is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:07.086756Z",
     "start_time": "2018-03-09T22:09:07.063321Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    votes = [0,1]\n",
    "    senators = np.random.choice(votes, replace=True, size=(100,4))\n",
    "    df = pd.DataFrame(senators, columns=['vote1','vote2','vote3','vote4'])\n",
    "    \n",
    "    def calculate_party(row):\n",
    "        x = row['vote1']\n",
    "        y = row['vote2']\n",
    "        z = row['vote3']\n",
    "\n",
    "        party = 0.7*x + 0.5*y - z + np.random.normal(0,0.3)\n",
    "        if party > 0.1:\n",
    "            return 'Dem'\n",
    "        elif party > 0.01:\n",
    "            return 'Ind'\n",
    "        else:\n",
    "            return 'Rep'\n",
    "    \n",
    "    df['party'] = df.apply(calculate_party,axis=1)\n",
    "    print(df.party.value_counts())\n",
    "    return df.iloc[:,:-1],df.iloc[:,-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:07.545056Z",
     "start_time": "2018-03-09T22:09:07.531704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dem    51\n",
      "Rep    48\n",
      "Ind     1\n",
      "Name: party, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:08.050081Z",
     "start_time": "2018-03-09T22:09:08.044046Z"
    }
   },
   "outputs": [],
   "source": [
    "nb = multinomial_naive_bayes()\n",
    "nb.fit(X.iloc[:90],y.iloc[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the probability of voting YES on each issue by what party the senators in our training data were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:09.097973Z",
     "start_time": "2018-03-09T22:09:09.091646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.multinomial_naive_bayes.__init__.<locals>.<lambda>>,\n",
       "            {'Dem': defaultdict(float,\n",
       "                         {0: 0.34444444444444444,\n",
       "                          1: 0.31111111111111112,\n",
       "                          2: 0.066666666666666666,\n",
       "                          3: 0.31111111111111112}),\n",
       "             'Ind': defaultdict(float,\n",
       "                         {0: 0.66666666666666663,\n",
       "                          1: 0.33333333333333331,\n",
       "                          2: 0.66666666666666663,\n",
       "                          3: 0.33333333333333331}),\n",
       "             'Rep': defaultdict(float,\n",
       "                         {0: 0.10465116279069768,\n",
       "                          1: 0.20930232558139536,\n",
       "                          2: 0.45348837209302323,\n",
       "                          3: 0.26744186046511625})})"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb._cond_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:10.399758Z",
     "start_time": "2018-03-09T22:09:10.393294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Dem'],\n",
       "       ['Rep']], \n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(X.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:11.089925Z",
     "start_time": "2018-03-09T22:09:11.082953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52534187],\n",
       "       [ 0.01279242],\n",
       "       [ 0.46186571],\n",
       "       [ 0.16880973],\n",
       "       [ 0.04110627],\n",
       "       [ 0.790084  ]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict_proba(X.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:11.666200Z",
     "start_time": "2018-03-09T22:09:11.658590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.8832252 ],\n",
       "       [-5.59842196],\n",
       "       [-2.01200026],\n",
       "       [-4.5912754 ],\n",
       "       [-6.00388707],\n",
       "       [-3.04790837]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict_log_proba(X.iloc[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an accuracy of 80%, which is the same as SkLearn's accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:13.370600Z",
     "start_time": "2018-03-09T22:09:13.363215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X.iloc[90:],y.iloc[90:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:14.014988Z",
     "start_time": "2018-03-09T22:09:14.003598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_sk = MultinomialNB()\n",
    "nb_sk.fit(X.iloc[:90],y.iloc[:90])\n",
    "nb_sk.score(X.iloc[90:],y.iloc[90:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:14.768890Z",
     "start_time": "2018-03-09T22:09:14.762391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.multinomial_naive_bayes.__init__.<locals>.<lambda>>,\n",
       "            {'Dem': defaultdict(float,\n",
       "                         {0: 0.34444444444444444,\n",
       "                          1: 0.31111111111111112,\n",
       "                          2: 0.066666666666666666,\n",
       "                          3: 0.31111111111111112}),\n",
       "             'Ind': defaultdict(float,\n",
       "                         {0: 0.66666666666666663,\n",
       "                          1: 0.33333333333333331,\n",
       "                          2: 0.66666666666666663,\n",
       "                          3: 0.33333333333333331}),\n",
       "             'Rep': defaultdict(float,\n",
       "                         {0: 0.10465116279069768,\n",
       "                          1: 0.20930232558139536,\n",
       "                          2: 0.45348837209302323,\n",
       "                          3: 0.26744186046511625})})"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb._cond_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:15.425372Z",
     "start_time": "2018-03-09T22:09:15.417681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33333333,  0.30107527,  0.06451613,  0.30107527],\n",
       "       [ 0.33333333,  0.16666667,  0.33333333,  0.16666667],\n",
       "       [ 0.1011236 ,  0.20224719,  0.43820225,  0.25842697]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(nb_sk.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:18.019001Z",
     "start_time": "2018-03-09T22:09:16.606721Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "from sklearn import datasets\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']\n",
    "ng_train = datasets.fetch_20newsgroups(subset='train', \n",
    "                                       categories=categories, \n",
    "                                       remove=('headers', \n",
    "                                               'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:42.240831Z",
     "start_time": "2018-03-09T22:09:42.043686Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=200,\n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6,\n",
    "                                   min_df = 3)\n",
    "\n",
    "X = count_vectorizer.fit_transform(ng_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:42.578698Z",
     "start_time": "2018-03-09T22:09:42.573097Z"
    }
   },
   "outputs": [],
   "source": [
    "X = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:42.957325Z",
     "start_time": "2018-03-09T22:09:42.940678Z"
    }
   },
   "outputs": [],
   "source": [
    "nb = multinomial_naive_bayes()\n",
    "nb.fit(X, ng_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:43.443817Z",
     "start_time": "2018-03-09T22:09:43.425791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:44.692823Z",
     "start_time": "2018-03-09T22:09:44.648925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] 2\n",
      "[0] 2\n",
      "[0] 0\n",
      "[0] 0\n",
      "[2] 2\n",
      "[0] 0\n",
      "[2] 2\n",
      "[1] 1\n",
      "[2] 1\n",
      "[2] 2\n",
      "[2] 0\n",
      "[1] 1\n",
      "[0] 0\n",
      "[0] 0\n",
      "[1] 1\n",
      "[1] 1\n",
      "[2] 2\n",
      "[0] 0\n",
      "[1] 1\n",
      "[1] 1\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(nb.predict(X[:20]),ng_train.target[:20]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:47.247925Z",
     "start_time": "2018-03-09T22:09:47.228430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.multinomial_naive_bayes.__init__.<locals>.<lambda>>,\n",
       "            {0: defaultdict(float,\n",
       "                         {0: 0.0028328611898016999,\n",
       "                          1: 0.0054703526423756964,\n",
       "                          2: 0.0011722184233662206,\n",
       "                          3: 0.0053726677737618439,\n",
       "                          4: 0.014555045423463905,\n",
       "                          5: 0.0034189704014848101,\n",
       "                          6: 0.019536973722770343,\n",
       "                          7: 0.009866171729999023,\n",
       "                          8: 0.015922633584057828,\n",
       "                          9: 0.00087916381752466545,\n",
       "                          10: 0.00048842434306925855,\n",
       "                          11: 0.00400507961316792,\n",
       "                          12: 0.00019536973722770342,\n",
       "                          13: 0.0056657223796033997,\n",
       "                          14: 0.009866171729999023,\n",
       "                          15: 0.01602031845267168,\n",
       "                          16: 0.0033212855328709585,\n",
       "                          17: 0.0057634072482172513,\n",
       "                          18: 0.011917553970889909,\n",
       "                          19: 0.0023444368467324412,\n",
       "                          20: 0.0024421217153462928,\n",
       "                          21: 0.0071309954088111752,\n",
       "                          22: 9.7684868613851712e-05,\n",
       "                          23: 0.0030282309270294031,\n",
       "                          24: 0.0060564618540588063,\n",
       "                          25: 0.00039073947445540685,\n",
       "                          26: 0.004786558562078734,\n",
       "                          27: 0.0013675881605939241,\n",
       "                          28: 0.00019536973722770342,\n",
       "                          29: 0.0033212855328709585,\n",
       "                          30: 0.0059587769854449546,\n",
       "                          31: 0.00068379408029696205,\n",
       "                          32: 0.0092800625183159131,\n",
       "                          33: 9.7684868613851712e-05,\n",
       "                          34: 0.00039073947445540685,\n",
       "                          35: 0.0042004493503956233,\n",
       "                          36: 0.00801015922633584,\n",
       "                          37: 0.0046888736934648824,\n",
       "                          38: 0.0067402559343557687,\n",
       "                          39: 0.00019536973722770342,\n",
       "                          40: 0.00019536973722770342,\n",
       "                          41: 0.020318452671681156,\n",
       "                          42: 0.0074240500146527302,\n",
       "                          43: 0.025691120445443,\n",
       "                          44: 0.0051772980365341406,\n",
       "                          45: 0.00068379408029696205,\n",
       "                          46: 0.0030282309270294031,\n",
       "                          47: 0.011722184233662206,\n",
       "                          48: 0.011136075021979096,\n",
       "                          49: 0.0093777473869297648,\n",
       "                          50: 0.00801015922633584,\n",
       "                          51: 0.0039073947445540684,\n",
       "                          52: 0.004786558562078734,\n",
       "                          53: 0.0011722184233662206,\n",
       "                          54: 0.0012699032919800722,\n",
       "                          55: 0.0029305460584155515,\n",
       "                          56: 9.7684868613851712e-05,\n",
       "                          57: 9.7684868613851712e-05,\n",
       "                          58: 0.0043958190876233275,\n",
       "                          59: 0.00068379408029696205,\n",
       "                          60: 0.0005861092116831103,\n",
       "                          61: 9.7684868613851712e-05,\n",
       "                          62: 9.7684868613851712e-05,\n",
       "                          63: 0.005079613167920289,\n",
       "                          64: 0.039660056657223795,\n",
       "                          65: 0.0035166552700986618,\n",
       "                          66: 0.011136075021979096,\n",
       "                          67: 0.0027351763211878482,\n",
       "                          68: 9.7684868613851712e-05,\n",
       "                          69: 0.0029305460584155515,\n",
       "                          70: 0.0036143401387125134,\n",
       "                          71: 0.0022467519781185896,\n",
       "                          72: 0.00029305460584155515,\n",
       "                          73: 0.0015629578978216274,\n",
       "                          74: 0.0005861092116831103,\n",
       "                          75: 0.00087916381752466545,\n",
       "                          76: 0.0029305460584155515,\n",
       "                          77: 0.0062518315912865096,\n",
       "                          78: 0.0005861092116831103,\n",
       "                          79: 0.00029305460584155515,\n",
       "                          80: 0.0014652730292077757,\n",
       "                          81: 0.00039073947445540685,\n",
       "                          82: 0.0030282309270294031,\n",
       "                          83: 0.010354596073068281,\n",
       "                          84: 0.0076194197518804335,\n",
       "                          85: 0.015238839503760867,\n",
       "                          86: 0.0038097098759402167,\n",
       "                          87: 9.7684868613851712e-05,\n",
       "                          88: 0.020513822408908859,\n",
       "                          89: 0.015141154635147015,\n",
       "                          90: 9.7684868613851712e-05,\n",
       "                          91: 0.004786558562078734,\n",
       "                          92: 0.0092800625183159131,\n",
       "                          93: 0.015922633584057828,\n",
       "                          94: 0.0016606427664354793,\n",
       "                          95: 0.0029305460584155515,\n",
       "                          96: 0.0055680375109895481,\n",
       "                          97: 0.0045911888248510307,\n",
       "                          98: 0.0060564618540588063,\n",
       "                          99: 0.0042981342190094758,\n",
       "                          100: 0.001074533554752369,\n",
       "                          101: 0.0014652730292077757,\n",
       "                          102: 0.0020513822408908858,\n",
       "                          103: 0.00068379408029696205,\n",
       "                          104: 0.0015629578978216274,\n",
       "                          105: 0.011331444759206799,\n",
       "                          106: 0.005861092116831103,\n",
       "                          107: 0.0046888736934648824,\n",
       "                          108: 0.0019536973722770342,\n",
       "                          109: 0.0075217348832665818,\n",
       "                          110: 0.004786558562078734,\n",
       "                          111: 0.0051772980365341406,\n",
       "                          112: 0.0055680375109895481,\n",
       "                          113: 0.0032236006642571064,\n",
       "                          114: 0.0051772980365341406,\n",
       "                          115: 0.0034189704014848101,\n",
       "                          116: 0.0030282309270294031,\n",
       "                          117: 0.0033212855328709585,\n",
       "                          118: 0.0039073947445540684,\n",
       "                          119: 9.7684868613851712e-05,\n",
       "                          120: 9.7684868613851712e-05,\n",
       "                          121: 0.032333691511184919,\n",
       "                          122: 0.0072286802774250269,\n",
       "                          123: 9.7684868613851712e-05,\n",
       "                          124: 0.00087916381752466545,\n",
       "                          125: 9.7684868613851712e-05,\n",
       "                          126: 9.7684868613851712e-05,\n",
       "                          127: 0.0086939533066328033,\n",
       "                          128: 0.00097684868613851709,\n",
       "                          129: 0.0057634072482172513,\n",
       "                          130: 0.0054703526423756964,\n",
       "                          131: 0.0024421217153462928,\n",
       "                          132: 0.0046888736934648824,\n",
       "                          133: 0.0044935039562371791,\n",
       "                          134: 9.7684868613851712e-05,\n",
       "                          135: 0.0007814789489108137,\n",
       "                          136: 0.001074533554752369,\n",
       "                          137: 0.00019536973722770342,\n",
       "                          138: 0.0005861092116831103,\n",
       "                          139: 0.0090846927810882099,\n",
       "                          140: 0.0049819282993064373,\n",
       "                          141: 0.00039073947445540685,\n",
       "                          142: 0.0078147894891081367,\n",
       "                          143: 0.0041027644817817717,\n",
       "                          144: 0.0084985835694051,\n",
       "                          145: 0.0053726677737618439,\n",
       "                          146: 0.015336524372374719,\n",
       "                          147: 0.011722184233662206,\n",
       "                          148: 0.0079124743577219884,\n",
       "                          149: 0.0007814789489108137,\n",
       "                          150: 0.00048842434306925855,\n",
       "                          151: 0.012894402657028426,\n",
       "                          152: 0.017094852007424052,\n",
       "                          153: 0.005861092116831103,\n",
       "                          154: 9.7684868613851712e-05,\n",
       "                          155: 9.7684868613851712e-05,\n",
       "                          156: 0.0020513822408908858,\n",
       "                          157: 0.0043958190876233275,\n",
       "                          158: 0.0022467519781185896,\n",
       "                          159: 0.0037120250073263651,\n",
       "                          160: 0.00029305460584155515,\n",
       "                          161: 0.001074533554752369,\n",
       "                          162: 0.0023444368467324412,\n",
       "                          163: 0.0023444368467324412,\n",
       "                          164: 0.0041027644817817717,\n",
       "                          165: 0.0037120250073263651,\n",
       "                          166: 0.004786558562078734,\n",
       "                          167: 0.0017583276350493309,\n",
       "                          168: 0.00029305460584155515,\n",
       "                          169: 0.0043958190876233275,\n",
       "                          170: 0.00097684868613851709,\n",
       "                          171: 0.0079124743577219884,\n",
       "                          172: 0.0097684868613851714,\n",
       "                          173: 0.021099931620591969,\n",
       "                          174: 0.0053726677737618439,\n",
       "                          175: 0.013285142131483834,\n",
       "                          176: 0.012894402657028426,\n",
       "                          177: 0.0061541467226726579,\n",
       "                          178: 0.004786558562078734,\n",
       "                          179: 0.0054703526423756964,\n",
       "                          180: 0.0070333105401973236,\n",
       "                          181: 0.0031259157956432548,\n",
       "                          182: 0.0035166552700986618,\n",
       "                          183: 0.0079124743577219884,\n",
       "                          184: 0.0029305460584155515,\n",
       "                          185: 0.00029305460584155515,\n",
       "                          186: 0.0045911888248510307,\n",
       "                          187: 0.0048842434306925857,\n",
       "                          188: 0.011819869102276058,\n",
       "                          189: 0.00048842434306925855,\n",
       "                          190: 9.7684868613851712e-05,\n",
       "                          191: 0.0017583276350493309,\n",
       "                          192: 0.0034189704014848101,\n",
       "                          193: 0.0077171046204942851,\n",
       "                          194: 0.0015629578978216274,\n",
       "                          195: 0.0032236006642571064,\n",
       "                          196: 0.006642571065741917,\n",
       "                          197: 0.0012699032919800722,\n",
       "                          198: 0.0046888736934648824,\n",
       "                          199: 0.0048842434306925857}),\n",
       "             1: defaultdict(float,\n",
       "                         {0: 0.0034845845011741536,\n",
       "                          1: 0.0026513142943716387,\n",
       "                          2: 0.005075373077797137,\n",
       "                          3: 0.00075751836982046819,\n",
       "                          4: 0.00022725551094614045,\n",
       "                          5: 0.0029543216422998259,\n",
       "                          6: 7.5751836982046808e-05,\n",
       "                          7: 0.00022725551094614045,\n",
       "                          8: 7.5751836982046808e-05,\n",
       "                          9: 0.014089841678660708,\n",
       "                          10: 0.00037875918491023409,\n",
       "                          11: 0.0014392849026588896,\n",
       "                          12: 7.5751836982046808e-05,\n",
       "                          13: 0.0067419134914021668,\n",
       "                          14: 7.5751836982046808e-05,\n",
       "                          15: 0.0011362775547307021,\n",
       "                          16: 0.0034845845011741536,\n",
       "                          17: 0.0048481175668509957,\n",
       "                          18: 7.5751836982046808e-05,\n",
       "                          19: 0.0018180440875691236,\n",
       "                          20: 0.011059768199378836,\n",
       "                          21: 0.0041663510340125744,\n",
       "                          22: 7.5751836982046808e-05,\n",
       "                          23: 0.0043178547079766681,\n",
       "                          24: 0.0024998106204075451,\n",
       "                          25: 0.0046966138928869021,\n",
       "                          26: 0.0010605257177486553,\n",
       "                          27: 0.0099234906446481336,\n",
       "                          28: 0.013256571471858193,\n",
       "                          29: 0.0065146579804560263,\n",
       "                          30: 0.0020452995985152638,\n",
       "                          31: 0.0096962351337019914,\n",
       "                          32: 0.0022725551094614042,\n",
       "                          33: 7.5751836982046808e-05,\n",
       "                          34: 0.016665404136050298,\n",
       "                          35: 0.0015150367396409364,\n",
       "                          36: 0.0018937959245511704,\n",
       "                          37: 0.0012120293917127489,\n",
       "                          38: 0.0046966138928869021,\n",
       "                          39: 0.0065146579804560263,\n",
       "                          40: 0.0096204832967199446,\n",
       "                          41: 0.011968790243163397,\n",
       "                          42: 0.0028785698053177791,\n",
       "                          43: 0.011817286569199304,\n",
       "                          44: 0.022574047420649952,\n",
       "                          45: 0.0057571396106355583,\n",
       "                          46: 0.0028785698053177791,\n",
       "                          47: 7.5751836982046808e-05,\n",
       "                          48: 0.0021968032724793574,\n",
       "                          49: 0.0004545110218922809,\n",
       "                          50: 0.0009090220437845618,\n",
       "                          51: 0.0035603363381562004,\n",
       "                          52: 0.0026513142943716387,\n",
       "                          53: 0.020149988637224454,\n",
       "                          54: 0.016513900462086204,\n",
       "                          55: 0.0036360881751382472,\n",
       "                          56: 0.01355957881978638,\n",
       "                          57: 0.0061358987955457923,\n",
       "                          58: 0.007878191046132868,\n",
       "                          59: 0.014392849026588895,\n",
       "                          60: 0.00068176653283842138,\n",
       "                          61: 0.00068176653283842138,\n",
       "                          62: 0.012196045754109538,\n",
       "                          63: 0.0020452995985152638,\n",
       "                          64: 0.00037875918491023409,\n",
       "                          65: 0.0021210514354973106,\n",
       "                          66: 0.0081054465570790084,\n",
       "                          67: 0.0036360881751382472,\n",
       "                          68: 0.03113400499962124,\n",
       "                          69: 0.002348306946443451,\n",
       "                          70: 0.0041663510340125744,\n",
       "                          71: 0.0087114612529353829,\n",
       "                          72: 0.0057571396106355583,\n",
       "                          73: 0.003711840012120294,\n",
       "                          74: 0.00037875918491023409,\n",
       "                          75: 0.00068176653283842138,\n",
       "                          76: 0.0025755624573895919,\n",
       "                          77: 0.0013635330656768428,\n",
       "                          78: 0.036739640936292707,\n",
       "                          79: 0.016135141277175972,\n",
       "                          80: 0.0046966138928869021,\n",
       "                          81: 0.0064389061434739795,\n",
       "                          82: 0.0099234906446481336,\n",
       "                          83: 7.5751836982046808e-05,\n",
       "                          84: 0.0011362775547307021,\n",
       "                          85: 7.5751836982046808e-05,\n",
       "                          86: 0.0017422922505870768,\n",
       "                          87: 0.020301492311188548,\n",
       "                          88: 0.0090144686008635701,\n",
       "                          89: 0.012574804939019772,\n",
       "                          90: 7.5751836982046808e-05,\n",
       "                          91: 0.0027270661313536855,\n",
       "                          92: 0.0010605257177486553,\n",
       "                          93: 0.012726308612983865,\n",
       "                          94: 0.0093932277857738042,\n",
       "                          95: 0.003711840012120294,\n",
       "                          96: 0.002348306946443451,\n",
       "                          97: 0.0055298840996894178,\n",
       "                          98: 0.0014392849026588896,\n",
       "                          99: 0.0049238694038330434,\n",
       "                          100: 0.0060601469585637455,\n",
       "                          101: 0.00053026285887432766,\n",
       "                          102: 0.003939095523066434,\n",
       "                          103: 0.0068934171653662604,\n",
       "                          104: 0.011211271873342929,\n",
       "                          105: 0.0041663510340125744,\n",
       "                          106: 0.0013635330656768428,\n",
       "                          107: 0.00060601469585637446,\n",
       "                          108: 0.0015150367396409364,\n",
       "                          109: 0.001287781228694796,\n",
       "                          110: 0.0021210514354973106,\n",
       "                          111: 0.0096962351337019914,\n",
       "                          112: 0.0062874024695098859,\n",
       "                          113: 0.0031815771532459664,\n",
       "                          114: 0.0036360881751382472,\n",
       "                          115: 0.0037875918491023408,\n",
       "                          116: 0.0067419134914021668,\n",
       "                          117: 0.0021210514354973106,\n",
       "                          118: 0.0041663510340125744,\n",
       "                          119: 0.010302249829558368,\n",
       "                          120: 0.0065146579804560263,\n",
       "                          121: 0.0041663510340125744,\n",
       "                          122: 0.00037875918491023409,\n",
       "                          123: 7.5751836982046808e-05,\n",
       "                          124: 0.00068176653283842138,\n",
       "                          125: 0.00060601469585637446,\n",
       "                          126: 0.00022725551094614045,\n",
       "                          127: 0.00666616165442012,\n",
       "                          128: 0.0061358987955457923,\n",
       "                          129: 0.0026513142943716387,\n",
       "                          130: 0.003939095523066434,\n",
       "                          131: 0.002348306946443451,\n",
       "                          132: 0.0029543216422998259,\n",
       "                          133: 0.0068176653283842136,\n",
       "                          134: 0.0069691690023483072,\n",
       "                          135: 0.015150367396409363,\n",
       "                          136: 0.007423680024240588,\n",
       "                          137: 0.012271797591091585,\n",
       "                          138: 0.0056813877736535115,\n",
       "                          139: 0.0037875918491023408,\n",
       "                          140: 0.0024998106204075451,\n",
       "                          141: 0.0062116506325278391,\n",
       "                          142: 0.0068176653283842136,\n",
       "                          143: 0.0041663510340125744,\n",
       "                          144: 0.0021968032724793574,\n",
       "                          145: 0.0009090220437845618,\n",
       "                          146: 7.5751836982046808e-05,\n",
       "                          147: 7.5751836982046808e-05,\n",
       "                          148: 0.0028028179683357323,\n",
       "                          149: 0.0052268767517612306,\n",
       "                          150: 0.0031815771532459664,\n",
       "                          151: 0.0017422922505870768,\n",
       "                          152: 0.0028028179683357323,\n",
       "                          153: 0.00075751836982046819,\n",
       "                          154: 0.0064389061434739795,\n",
       "                          155: 7.5751836982046808e-05,\n",
       "                          156: 0.0020452995985152638,\n",
       "                          157: 0.001287781228694796,\n",
       "                          158: 0.0080296947200969616,\n",
       "                          159: 0.0053026285887432774,\n",
       "                          160: 0.016135141277175972,\n",
       "                          161: 0.0056813877736535115,\n",
       "                          162: 0.0017422922505870768,\n",
       "                          163: 0.0046208620559048553,\n",
       "                          164: 0.0022725551094614042,\n",
       "                          165: 0.0065904098174380732,\n",
       "                          166: 0.0035603363381562004,\n",
       "                          167: 0.0065904098174380732,\n",
       "                          168: 0.00022725551094614045,\n",
       "                          169: 0.0022725551094614042,\n",
       "                          170: 0.010681009014468602,\n",
       "                          171: 0.0025755624573895919,\n",
       "                          172: 0.0028028179683357323,\n",
       "                          173: 0.0065146579804560263,\n",
       "                          174: 0.0012120293917127489,\n",
       "                          175: 0.0087872130899174297,\n",
       "                          176: 0.0024998106204075451,\n",
       "                          177: 0.003711840012120294,\n",
       "                          178: 0.0015150367396409364,\n",
       "                          179: 0.017119915157942579,\n",
       "                          180: 0.007423680024240588,\n",
       "                          181: 0.0099992424816301804,\n",
       "                          182: 0.002348306946443451,\n",
       "                          183: 0.0067419134914021668,\n",
       "                          184: 0.011135520036360883,\n",
       "                          185: 0.00666616165442012,\n",
       "                          186: 0.0027270661313536855,\n",
       "                          187: 0.007044920839330354,\n",
       "                          188: 0.0049238694038330434,\n",
       "                          189: 0.00022725551094614045,\n",
       "                          190: 0.007423680024240588,\n",
       "                          191: 0.0011362775547307021,\n",
       "                          192: 0.0071964245132944476,\n",
       "                          193: 0.0022725551094614042,\n",
       "                          194: 0.0049996212408150902,\n",
       "                          195: 0.0034845845011741536,\n",
       "                          196: 0.0011362775547307021,\n",
       "                          197: 0.0014392849026588896,\n",
       "                          198: 0.0027270661313536855,\n",
       "                          199: 0.0022725551094614042}),\n",
       "             2: defaultdict(float,\n",
       "                         {0: 0.00244140625,\n",
       "                          1: 0.0059814453125,\n",
       "                          2: 0.001220703125,\n",
       "                          3: 0.0025634765625,\n",
       "                          4: 0.0023193359375,\n",
       "                          5: 0.002197265625,\n",
       "                          6: 0.0001220703125,\n",
       "                          7: 0.0001220703125,\n",
       "                          8: 0.0001220703125,\n",
       "                          9: 0.001220703125,\n",
       "                          10: 0.0108642578125,\n",
       "                          11: 0.0079345703125,\n",
       "                          12: 0.016845703125,\n",
       "                          13: 0.001708984375,\n",
       "                          14: 0.0001220703125,\n",
       "                          15: 0.0081787109375,\n",
       "                          16: 0.0098876953125,\n",
       "                          17: 0.0172119140625,\n",
       "                          18: 0.0001220703125,\n",
       "                          19: 0.006591796875,\n",
       "                          20: 0.003662109375,\n",
       "                          21: 0.00146484375,\n",
       "                          22: 0.0107421875,\n",
       "                          23: 0.0015869140625,\n",
       "                          24: 0.0029296875,\n",
       "                          25: 0.002197265625,\n",
       "                          26: 0.0030517578125,\n",
       "                          27: 0.000244140625,\n",
       "                          28: 0.000732421875,\n",
       "                          29: 0.00244140625,\n",
       "                          30: 0.00537109375,\n",
       "                          31: 0.000244140625,\n",
       "                          32: 0.00439453125,\n",
       "                          33: 0.01220703125,\n",
       "                          34: 0.0013427734375,\n",
       "                          35: 0.008544921875,\n",
       "                          36: 0.0113525390625,\n",
       "                          37: 0.0076904296875,\n",
       "                          38: 0.0023193359375,\n",
       "                          39: 0.0001220703125,\n",
       "                          40: 0.000244140625,\n",
       "                          41: 0.0096435546875,\n",
       "                          42: 0.0084228515625,\n",
       "                          43: 0.0228271484375,\n",
       "                          44: 0.0064697265625,\n",
       "                          45: 0.0003662109375,\n",
       "                          46: 0.004638671875,\n",
       "                          47: 0.0008544921875,\n",
       "                          48: 0.0018310546875,\n",
       "                          49: 0.0001220703125,\n",
       "                          50: 0.00341796875,\n",
       "                          51: 0.0001220703125,\n",
       "                          52: 0.0054931640625,\n",
       "                          53: 0.0003662109375,\n",
       "                          54: 0.0003662109375,\n",
       "                          55: 0.001220703125,\n",
       "                          56: 0.000244140625,\n",
       "                          57: 0.0001220703125,\n",
       "                          58: 0.0023193359375,\n",
       "                          59: 0.0001220703125,\n",
       "                          60: 0.0250244140625,\n",
       "                          61: 0.018310546875,\n",
       "                          62: 0.0006103515625,\n",
       "                          63: 0.0037841796875,\n",
       "                          64: 0.0009765625,\n",
       "                          65: 0.0079345703125,\n",
       "                          66: 0.0245361328125,\n",
       "                          67: 0.0107421875,\n",
       "                          68: 0.0001220703125,\n",
       "                          69: 0.0072021484375,\n",
       "                          70: 0.0013427734375,\n",
       "                          71: 0.0030517578125,\n",
       "                          72: 0.000732421875,\n",
       "                          73: 0.0037841796875,\n",
       "                          74: 0.016845703125,\n",
       "                          75: 0.0108642578125,\n",
       "                          76: 0.0029296875,\n",
       "                          77: 0.0003662109375,\n",
       "                          78: 0.0001220703125,\n",
       "                          79: 0.0001220703125,\n",
       "                          80: 0.0010986328125,\n",
       "                          81: 0.0009765625,\n",
       "                          82: 0.00146484375,\n",
       "                          83: 0.0001220703125,\n",
       "                          84: 0.0042724609375,\n",
       "                          85: 0.0001220703125,\n",
       "                          86: 0.00390625,\n",
       "                          87: 0.000244140625,\n",
       "                          88: 0.019775390625,\n",
       "                          89: 0.0140380859375,\n",
       "                          90: 0.0145263671875,\n",
       "                          91: 0.00634765625,\n",
       "                          92: 0.0009765625,\n",
       "                          93: 0.018798828125,\n",
       "                          94: 0.00341796875,\n",
       "                          95: 0.0047607421875,\n",
       "                          96: 0.005126953125,\n",
       "                          97: 0.009765625,\n",
       "                          98: 0.00634765625,\n",
       "                          99: 0.006103515625,\n",
       "                          100: 0.0030517578125,\n",
       "                          101: 0.01123046875,\n",
       "                          102: 0.0084228515625,\n",
       "                          103: 0.0003662109375,\n",
       "                          104: 0.0032958984375,\n",
       "                          105: 0.0089111328125,\n",
       "                          106: 0.0025634765625,\n",
       "                          107: 0.003662109375,\n",
       "                          108: 0.0062255859375,\n",
       "                          109: 0.0042724609375,\n",
       "                          110: 0.00146484375,\n",
       "                          111: 0.00390625,\n",
       "                          112: 0.0106201171875,\n",
       "                          113: 0.00244140625,\n",
       "                          114: 0.0006103515625,\n",
       "                          115: 0.0023193359375,\n",
       "                          116: 0.0030517578125,\n",
       "                          117: 0.0040283203125,\n",
       "                          118: 0.0008544921875,\n",
       "                          119: 0.000244140625,\n",
       "                          120: 0.000244140625,\n",
       "                          121: 0.00732421875,\n",
       "                          122: 0.0015869140625,\n",
       "                          123: 0.0126953125,\n",
       "                          124: 0.0101318359375,\n",
       "                          125: 0.01025390625,\n",
       "                          126: 0.0166015625,\n",
       "                          127: 0.006103515625,\n",
       "                          128: 0.003173828125,\n",
       "                          129: 0.0020751953125,\n",
       "                          130: 0.0029296875,\n",
       "                          131: 0.0064697265625,\n",
       "                          132: 0.0067138671875,\n",
       "                          133: 0.001953125,\n",
       "                          134: 0.0001220703125,\n",
       "                          135: 0.0003662109375,\n",
       "                          136: 0.0001220703125,\n",
       "                          137: 0.0001220703125,\n",
       "                          138: 0.0009765625,\n",
       "                          139: 0.0020751953125,\n",
       "                          140: 0.0028076171875,\n",
       "                          141: 0.00048828125,\n",
       "                          142: 0.0028076171875,\n",
       "                          143: 0.00244140625,\n",
       "                          144: 0.0113525390625,\n",
       "                          145: 0.0032958984375,\n",
       "                          146: 0.0001220703125,\n",
       "                          147: 0.0001220703125,\n",
       "                          148: 0.0087890625,\n",
       "                          149: 0.0103759765625,\n",
       "                          150: 0.016845703125,\n",
       "                          151: 0.0057373046875,\n",
       "                          152: 0.00830078125,\n",
       "                          153: 0.00341796875,\n",
       "                          154: 0.000244140625,\n",
       "                          155: 0.0137939453125,\n",
       "                          156: 0.009521484375,\n",
       "                          157: 0.0030517578125,\n",
       "                          158: 0.001953125,\n",
       "                          159: 0.000732421875,\n",
       "                          160: 0.000244140625,\n",
       "                          161: 0.0008544921875,\n",
       "                          162: 0.0048828125,\n",
       "                          163: 0.00146484375,\n",
       "                          164: 0.0015869140625,\n",
       "                          165: 0.00048828125,\n",
       "                          166: 0.0057373046875,\n",
       "                          167: 0.0001220703125,\n",
       "                          168: 0.02392578125,\n",
       "                          169: 0.0037841796875,\n",
       "                          170: 0.0050048828125,\n",
       "                          171: 0.004638671875,\n",
       "                          172: 0.0030517578125,\n",
       "                          173: 0.023193359375,\n",
       "                          174: 0.00390625,\n",
       "                          175: 0.01611328125,\n",
       "                          176: 0.00244140625,\n",
       "                          177: 0.0013427734375,\n",
       "                          178: 0.0020751953125,\n",
       "                          179: 0.00390625,\n",
       "                          180: 0.00537109375,\n",
       "                          181: 0.00146484375,\n",
       "                          182: 0.0020751953125,\n",
       "                          183: 0.0079345703125,\n",
       "                          184: 0.0003662109375,\n",
       "                          185: 0.000244140625,\n",
       "                          186: 0.0003662109375,\n",
       "                          187: 0.0042724609375,\n",
       "                          188: 0.0091552734375,\n",
       "                          189: 0.0133056640625,\n",
       "                          190: 0.0001220703125,\n",
       "                          191: 0.0152587890625,\n",
       "                          192: 0.0035400390625,\n",
       "                          193: 0.003662109375,\n",
       "                          194: 0.000244140625,\n",
       "                          195: 0.0006103515625,\n",
       "                          196: 0.0032958984375,\n",
       "                          197: 0.0379638671875,\n",
       "                          198: 0.01171875,\n",
       "                          199: 0.0035400390625})})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb._cond_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:48.991875Z",
     "start_time": "2018-03-09T22:09:48.865276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X[:100],ng_train.target[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:49.995562Z",
     "start_time": "2018-03-09T22:09:49.985310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_sk = MultinomialNB()\n",
    "nb_sk.fit(X, ng_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:09:50.470684Z",
     "start_time": "2018-03-09T22:09:50.462618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85999999999999999"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_sk.score(X[:100], ng_train.target[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
